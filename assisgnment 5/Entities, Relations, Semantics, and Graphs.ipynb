{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Discussed with Animesh Sagar and Chaitanya Patil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "e6CXmEzB4XBx",
    "outputId": "242d90f0-5d57-45f2-c98e-42b23b0a3e0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wmd in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from wmd) (1.17.4)\n",
      "Requirement already satisfied: textacy in /usr/local/lib/python3.6/dist-packages (0.9.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.4)\n",
      "Requirement already satisfied: pyemd>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.5.1)\n",
      "Requirement already satisfied: cachetools>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from textacy) (3.1.1)\n",
      "Requirement already satisfied: pyphen>=0.9.4 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.9.5)\n",
      "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.21.0)\n",
      "Requirement already satisfied: srsly>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.2.0)\n",
      "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.14.1)\n",
      "Requirement already satisfied: spacy>=2.0.12 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.1.9)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.17.4)\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.21.3)\n",
      "Requirement already satisfied: jellyfish>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.7.2)\n",
      "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.6/dist-packages (from textacy) (4.28.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.3.3)\n",
      "Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.10.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->textacy) (4.4.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (2019.11.28)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (0.9.6)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (2.0.1)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (7.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (2.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (0.4.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (1.0.2)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (0.2.4)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz>=0.8.0->textacy) (0.10.0)\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import wmd\n",
    "import warnings\n",
    "import en_core_web_sm\n",
    "import textacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "download('stopwords')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ggwnOf1aBeXG"
   },
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "cOojpvcFDSR5",
    "outputId": "ab5dbe7b-f3d5-44a1-9e1a-28331798abe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence \t    Similarity Score\n",
      "This is a sentence. 0.40717865473987574\n",
      "This is another sentence. 0.40717865473987574\n",
      "John likes milk. 0.46725571131126653\n",
      "Peter eats candy. 0.5662843523571846\n",
      "My dog likes bones.\n",
      " 0.460992989580924\n",
      "\n",
      "The sentence in the text file most similar to seed sentence is : Peter eats candy.\n"
     ]
    }
   ],
   "source": [
    "# ref: https://github.com/dcavar/python-tutorial-for-ipython/blob/master/notebooks/spaCy%20Tutorial.ipynb\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "x=[]\n",
    "nlp = spacy.load('en', create_pipeline=wmd.WMD.create_spacy_pipeline)\n",
    "with open('ex1.txt', 'r') as f:\n",
    "    data_read= f.readline()\n",
    "\n",
    "seed_sentence = nlp(\"My poodle likes food in general.\")\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = data_read\n",
    "text_tokens = nlp(text)\n",
    "\n",
    "for sentence in text_tokens.sents:\n",
    "    x.append(nlp(sentence.text))\n",
    "\n",
    "print(\"Sentence \\t    Similarity Score\")\n",
    "\n",
    "for i in range(len(x)):\n",
    "  sample=x[i]\n",
    "  sample_no_stop = nlp(' '.join([str(t) for t in sample if not t.is_stop]))\n",
    "  sample_pron = nlp(' '.join([str(t) for t in sample_no_stop if t.pos_ in ['NOUN', 'PROPN']] ))\n",
    "  print( sample,  sample_pron.similarity(seed_sentence))\n",
    "\n",
    "print(\"\\nThe sentence in the text file most similar to seed sentence is :\" , x[3])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gXeLGSYQ9myB"
   },
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5SKgPvyk5SyR"
   },
   "outputs": [],
   "source": [
    "text1=nlp(u'John loves apples')\n",
    "text2=nlp(u'some man likes fruit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "RePnG7kF9way",
    "outputId": "44981c20-e4bc-4c63-e1b7-f5fe93464c83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John some 0.16244455\n",
      "John man 0.27093855\n",
      "John likes -0.016569503\n",
      "John fruit 0.2554266\n",
      "loves some 0.03814313\n",
      "loves man 0.11364341\n",
      "loves likes 0.5741299\n",
      "loves fruit 0.03479158\n",
      "apples some 0.017183943\n",
      "apples man 0.15219189\n",
      "apples likes 0.48223904\n",
      "apples fruit 0.29982755\n",
      " \n",
      "Max distance_list found between 'loves' & 'likes' :  0.5741299\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "distance_list = []\n",
    "\n",
    "for token1 in text1:\n",
    "    for token2 in text2:\n",
    "        print(token1, token2, token1.similarity(token2))\n",
    "        distance_list.append(token1.similarity(token2))\n",
    "\n",
    "print(\" \\nMax distance_list found between 'loves' & 'likes' : \", max(distance_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S-MAIpFxA3Ic",
    "outputId": "f3c07318-bdb0-4e11-8d18-8517b80991b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein distance between words :  2\n"
     ]
    }
   ],
   "source": [
    "# ref : https://www.python-course.eu/levenshtein_distance.php\n",
    "def lev_dist(s, t):\n",
    "    if s == \"\":\n",
    "        return len(t)\n",
    "    if t == \"\":\n",
    "        return len(s)\n",
    "    if s[-1] == t[-1]:\n",
    "        cost = 0\n",
    "    else:\n",
    "        cost = 1\n",
    "       \n",
    "    res = min([lev_dist(s[:-1], t)+1,\n",
    "               lev_dist(s, t[:-1])+1, \n",
    "               lev_dist(s[:-1], t[:-1]) + cost])\n",
    "\n",
    "    return res\n",
    "\n",
    "print(\"Levenshtein distance between words : \", lev_dist(\"loves\", \"likes\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AIFw4ci_NfEs",
    "outputId": "413ec59e-fd93-41ed-f30a-f3b46478a5cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word mover's mean_dist is: 4.333333333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean_dist=[]\n",
    "for i in text1:\n",
    "  for j in text2:\n",
    "    mean_dist.append(lev_dist(str(i),str(j)))\n",
    "    \n",
    "print(\"The word mover's mean_dist is:\" , np.mean(mean_dist))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VM1dzJuUJFMY"
   },
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "19oqkEjTPmg0",
    "outputId": "9c3dbb7d-b982-4603-e8d6-0142315df567"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(The Pentagon has suspended operational training for all Saudi military students in the United States, indefinitely halting flight instruction, firing range training and all other operations outside the classroom in the wake of a shooting last week at Naval Air Station Pensacola in Florida by a member of the Saudi Royal Air Force., ([Pentagon], [suspended], [training, students, States, instruction, training, classroom, wake, shooting, Pensacola, Florida, member, Force]))\n",
      "(The suspension will affect nearly 900 Saudi students across the country, the Defense Department said on Tuesday., ([suspension, Department], [said], [students, country, Tuesday]))\n",
      "(Classroom teaching, including language courses, will continue while Pentagon leaders review vetting procedures for all foreign military trainees., ([teaching, leaders], [continue], [courses, procedures, trainees]))\n",
      "(An estimated 5,200 international students in the United States will be covered by the security review., ([students], [covered], [States, review]))\n",
      "(The “safety stand-down” was issued pending the results of an F.B.I. investigation into the shooting on Friday that left three young sailors dead and eight other people wounded., ([down, that, people], [issued], [results, investigation, shooting, Friday, sailors]))\n",
      "(Several lawmakers, including Senator Rick Scott of Florida and Representative Matt Gaetz, whose congressional district includes Pensacola, had called for a review of foreign military programs and their screening process., ([lawmakers, district], [called], [Scott, Florida, Pensacola, review, programs]))\n",
      "(The suspension of operational training for hundreds of Saudi military students is an extraordinary rebuke by the Pentagon, especially at a time when President Trump has tamped down suggestions that the Saudi government must be held to account on an array of recent issues., ([suspension, Trump, government], [is], [training, hundreds, students, Pentagon, time, suggestions, array, issues]))\n",
      "(Even before the shooting on Friday, the White House had been fighting efforts in Congress to cut military aid to the Saudis, a reflection of anger over the continuing war in Yemen and the brutal killing in Istanbul of Jamal Khashoggi, a Saudi dissident and journalist who had been granted legal residence in the United States., ([House, who], [fighting], [shooting, Friday, efforts, Congress, aid, Saudis, anger, war, Yemen, Istanbul, Khashoggi, residence, States]))\n",
      "(American intelligence findings closely tie Saudi Arabia’s crown prince and de facto leader, Mohammed bin Salman, to the murder., ([findings], [tie], [’s, prince, murder]))\n",
      "(Senior Defense Department officials, speaking to reporters in a hastily organized conference call on Tuesday night, insisted that suspending operational training for students from Saudi Arabia — the only country singled out for a broader review of security procedures governing the international military students — would be short-term and would not upset the strategic relationship between the two countries., ([officials, country], [insisted], [reporters, call, night, training, students, Arabia, review, procedures, students, relationship, countries]))\n",
      "(Defense Secretary Mark Esper called his Saudi counterpart, Khalid bin Salman, to discuss the new limitations on Saudi military students, who are now essentially restricted to classroom training like English language courses., ([Esper, who], [called], [counterpart, limitations, students, training, courses]))\n",
      "(It was unclear whether the review of security and vetting procedures means that federal investigators have found something troubling, or whether it was merely a precautionary measure., ([It, review, investigators, something, it], [was], [procedures]))\n",
      "(Lawmakers praised the Defense Department’s action on the Saudi trainees.“At this point, suspending training just makes sense,” said Representative Elissa Slotkin, a Democrat of Michigan and a former Pentagon official., ([Lawmakers, Slotkin], [said], [action, trainees.“At, training, sense, Michigan]))\n",
      "(“, ([], [“], []))\n",
      "(We have to maintain the utmost, serious security standards — and obviously something has gone deeply awry.”The Navy had announced earlier on Tuesday that about 300 Saudi aviation students would be grounded at three bases in Florida., ([We, something, Navy, students], [have], [—, Tuesday, bases, Florida]))\n",
      "(The Pentagon later released a memo from David L. Norquist, the deputy secretary of defense, clarifying that the suspension would apply to 852 Saudis enrolled in all military training programs., ([Pentagon, suspension, Saudis], [released], [memo, Norquist, defense, programs]))\n",
      "(The memo described Saudi Arabia an “essential partner” that is working closely with the United States to investigate the shooting.“The Department has trained more than 28,000 Saudi students over the life of our security cooperation relationship without serious incident,” Mr. Norquist wrote., ([memo, that, Norquist], [wrote], [Arabia, partner, States, Department, students, life, relationship, incident]))\n"
     ]
    }
   ],
   "source": [
    "new_list=[]\n",
    "\n",
    "SUBJ = [\"nsubj\",\"nsubjpass\"] \n",
    "VERB = [\"ROOT\"] \n",
    "OBJ = [\"dobj\", \"pobj\"] \n",
    "\n",
    "with open('samplefile.txt', 'r') as f:\n",
    "    data_my = f.readline()\n",
    "\n",
    "final_sentence = nlp(data_my)\n",
    "\n",
    "for i in final_sentence.sents:\n",
    "    new_list.append(nlp(i.text))\n",
    "\n",
    "def svo(sentence):\n",
    "  text_my = nlp(str(sentence))\n",
    "  sub_toks = [tok for tok in text_my if (tok.dep_ in SUBJ) ]\n",
    "  obj_toks = [tok for tok in text_my if (tok.dep_ in OBJ) ]\n",
    "  vrb_toks = [tok for tok in text_my if (tok.dep_ in VERB) ]\n",
    "  return sub_toks, vrb_toks, obj_toks\n",
    "\n",
    "\n",
    "\n",
    "nlist=[]\n",
    "for i in range(len(new_list)):\n",
    "  spacy_svo_output= (new_list[i], svo(str(new_list[i])))\n",
    "  nlist.append(spacy_svo_output)\n",
    "  print(spacy_svo_output)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mB3iQHdIYgWb"
   },
   "outputs": [],
   "source": [
    "with open(\"output.txt\",\"w+\") as f:\n",
    "    f.write(str(nlist))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2EcOakdS9-YB"
   },
   "source": [
    "# Steps\n",
    "\n",
    "1. Install Spacy\n",
    "2. Import sample text file and tokenize into sentences.\n",
    "3. Print Subject, Verb, Object triplets if the word tokens exists in \"nsubj\", \"nsubjpass\", \"Root\", \"pobj\", \"dobj\".\n",
    "4. Write the output into text file."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ANLP_last.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
